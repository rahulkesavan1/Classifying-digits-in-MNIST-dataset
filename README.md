# Classifying-digits-in-MNIST-dataset
- Designed a neural network from scratch in python, implementing backpropogation
- Included an optional argument for cross entropy cost to account for learning slowdown caused by quadratic cost
- Implemented L2 regularization to account for overfitting, improving classification accuracy on test data
- Applied the neural network on the MNIST dataset of handwritten digits, achieving a 97.46% classification accuracy 
- Modified the network to utilize ReLU activations, achieving greater convergence rates and a 96.75% classification accurac

Note: Neural Network can be found in network.ipynb, and implementation on MNIST dataset can be found in MNIST.ipynb
ReLU implimentation can be found in ReLU MNIST
